"""
Mask Processor for World2Filter.

Handles preprocessing, caching, and management of segmentation masks
generated by SAM3 for foreground/background separation.
"""

import os
import hashlib
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Union

import numpy as np
import torch
from torch import Tensor

from src.models.segmentation.sam3_wrapper import SAM3Wrapper, create_sam3_wrapper


class MaskProcessor:
    """
    Processor for segmentation masks.
    
    Handles:
    - Batch mask generation from observations
    - Disk caching for efficient reuse
    - Mask preprocessing (smoothing, resizing)
    - Integration with replay buffer
    """
    
    def __init__(
        self,
        cache_dir: Optional[Union[str, Path]] = None,
        prompt: str = "robot",
        device: str = "cuda",
        use_sam3: bool = True,
        smooth_masks: bool = True,
        smooth_kernel: int = 5,
    ):
        """
        Args:
            cache_dir: Directory for caching masks
            prompt: Text prompt for SAM3
            device: Device for SAM3 inference
            use_sam3: Whether to use SAM3 or fallback
            smooth_masks: Whether to smooth mask edges
            smooth_kernel: Kernel size for Gaussian smoothing
        """
        self.cache_dir = Path(cache_dir) if cache_dir else None
        self.prompt = prompt
        self.device = device
        self.smooth_masks = smooth_masks
        self.smooth_kernel = smooth_kernel
        
        # Initialize SAM3 wrapper
        self.sam3 = create_sam3_wrapper(
            use_sam3=use_sam3,
            device=device,
            prompt=prompt,
        )
        
        # In-memory cache
        self._memory_cache: Dict[str, Tuple[np.ndarray, np.ndarray]] = {}
        self._cache_hits = 0
        self._cache_misses = 0
        
        # Create cache directory
        if self.cache_dir:
            self.cache_dir.mkdir(parents=True, exist_ok=True)
    
    def process_observation(
        self,
        obs: Union[np.ndarray, Tensor],
    ) -> Tuple[np.ndarray, np.ndarray]:
        """
        Process single observation to get masks.
        
        Args:
            obs: Observation (C, H, W) or (H, W, C)
        
        Returns:
            Tuple of (fg_mask, bg_mask)
        """
        # Check cache first
        cache_key = self._compute_key(obs)
        
        # Memory cache
        if cache_key in self._memory_cache:
            self._cache_hits += 1
            return self._memory_cache[cache_key]
        
        # Disk cache
        if self.cache_dir:
            cached = self._load_from_disk(cache_key)
            if cached is not None:
                self._cache_hits += 1
                self._memory_cache[cache_key] = cached
                return cached
        
        self._cache_misses += 1
        
        # Generate masks
        fg_mask, bg_mask = self.sam3.segment(obs, self.prompt)
        
        # Post-process
        if self.smooth_masks:
            fg_mask = self._smooth_mask(fg_mask)
            bg_mask = self._smooth_mask(bg_mask)
        
        # Cache result
        result = (fg_mask, bg_mask)
        self._memory_cache[cache_key] = result
        
        if self.cache_dir:
            self._save_to_disk(cache_key, result)
        
        return result
    
    def process_batch(
        self,
        obs_batch: Union[np.ndarray, Tensor],
    ) -> Tuple[np.ndarray, np.ndarray]:
        """
        Process batch of observations to get masks.
        
        Args:
            obs_batch: Batch of observations (N, C, H, W) or (N, H, W, C)
        
        Returns:
            Tuple of (fg_masks, bg_masks) as (N, H, W) arrays
        """
        if isinstance(obs_batch, Tensor):
            obs_batch = obs_batch.detach().cpu().numpy()
        
        fg_masks = []
        bg_masks = []
        
        for obs in obs_batch:
            fg, bg = self.process_observation(obs)
            fg_masks.append(fg)
            bg_masks.append(bg)
        
        return np.stack(fg_masks), np.stack(bg_masks)
    
    def process_episode(
        self,
        observations: Union[np.ndarray, Tensor],
        save_to_cache: bool = True,
    ) -> Tuple[np.ndarray, np.ndarray]:
        """
        Process all observations in an episode.
        
        Args:
            observations: Episode observations (T, C, H, W)
            save_to_cache: Whether to save to disk cache
        
        Returns:
            Tuple of (fg_masks, bg_masks) as (T, H, W) arrays
        """
        return self.process_batch(observations)
    
    def precompute_dataset(
        self,
        observations: Union[np.ndarray, Tensor, List[np.ndarray]],
        save_dir: Optional[Union[str, Path]] = None,
        batch_size: int = 32,
        verbose: bool = True,
    ):
        """
        Precompute masks for an entire dataset.
        
        Args:
            observations: All observations to process
            save_dir: Directory to save masks
            batch_size: Processing batch size
            verbose: Print progress
        """
        save_dir = Path(save_dir) if save_dir else self.cache_dir
        if save_dir is None:
            raise ValueError("No save directory specified")
        
        save_dir.mkdir(parents=True, exist_ok=True)
        
        if isinstance(observations, list):
            observations = np.concatenate(observations, axis=0)
        elif isinstance(observations, Tensor):
            observations = observations.detach().cpu().numpy()
        
        n_total = len(observations)
        n_batches = (n_total + batch_size - 1) // batch_size
        
        if verbose:
            print(f"Precomputing masks for {n_total} observations...")
        
        all_fg_masks = []
        all_bg_masks = []
        
        for i in range(n_batches):
            start = i * batch_size
            end = min(start + batch_size, n_total)
            batch = observations[start:end]
            
            fg_masks, bg_masks = self.process_batch(batch)
            all_fg_masks.append(fg_masks)
            all_bg_masks.append(bg_masks)
            
            if verbose and (i + 1) % 10 == 0:
                print(f"  Processed {end}/{n_total} observations")
        
        # Concatenate all masks
        all_fg_masks = np.concatenate(all_fg_masks, axis=0)
        all_bg_masks = np.concatenate(all_bg_masks, axis=0)
        
        # Save to disk
        np.savez_compressed(
            save_dir / "masks.npz",
            fg_masks=all_fg_masks,
            bg_masks=all_bg_masks,
        )
        
        if verbose:
            print(f"Saved masks to {save_dir / 'masks.npz'}")
            print(f"Cache stats: {self._cache_hits} hits, {self._cache_misses} misses")
    
    def load_precomputed(
        self,
        path: Union[str, Path],
    ) -> Tuple[np.ndarray, np.ndarray]:
        """
        Load precomputed masks from disk.
        
        Args:
            path: Path to masks.npz file
        
        Returns:
            Tuple of (fg_masks, bg_masks)
        """
        data = np.load(path)
        return data['fg_masks'], data['bg_masks']
    
    def _compute_key(self, obs: Union[np.ndarray, Tensor]) -> str:
        """Compute unique key for observation."""
        if isinstance(obs, Tensor):
            obs = obs.detach().cpu().numpy()
        
        # Use hash of observation bytes
        return hashlib.md5(obs.tobytes()).hexdigest()
    
    def _smooth_mask(self, mask: np.ndarray) -> np.ndarray:
        """Apply Gaussian smoothing to mask."""
        import cv2
        
        if self.smooth_kernel > 0:
            mask = cv2.GaussianBlur(
                mask.astype(np.float32),
                (self.smooth_kernel, self.smooth_kernel),
                0
            )
        
        return mask
    
    def _save_to_disk(
        self,
        key: str,
        masks: Tuple[np.ndarray, np.ndarray],
    ):
        """Save masks to disk cache."""
        if self.cache_dir is None:
            return
        
        path = self.cache_dir / f"{key}.npz"
        np.savez_compressed(path, fg=masks[0], bg=masks[1])
    
    def _load_from_disk(
        self,
        key: str,
    ) -> Optional[Tuple[np.ndarray, np.ndarray]]:
        """Load masks from disk cache."""
        if self.cache_dir is None:
            return None
        
        path = self.cache_dir / f"{key}.npz"
        if not path.exists():
            return None
        
        data = np.load(path)
        return data['fg'], data['bg']
    
    def clear_memory_cache(self):
        """Clear in-memory cache."""
        self._memory_cache.clear()
        self._cache_hits = 0
        self._cache_misses = 0
    
    def clear_disk_cache(self):
        """Clear disk cache."""
        if self.cache_dir and self.cache_dir.exists():
            for f in self.cache_dir.glob("*.npz"):
                f.unlink()
    
    @property
    def cache_stats(self) -> Dict[str, int]:
        """Get cache statistics."""
        return {
            'hits': self._cache_hits,
            'misses': self._cache_misses,
            'hit_rate': self._cache_hits / max(1, self._cache_hits + self._cache_misses),
            'memory_cache_size': len(self._memory_cache),
        }


class OnlineMaskProcessor:
    """
    Mask processor for online (during training) mask generation.
    
    Processes observations as they come in during environment interaction.
    """
    
    def __init__(
        self,
        processor: MaskProcessor,
        buffer_size: int = 1000,
    ):
        """
        Args:
            processor: Base mask processor
            buffer_size: Size of observation buffer
        """
        self.processor = processor
        self.buffer_size = buffer_size
        
        self._obs_buffer: List[np.ndarray] = []
        self._mask_buffer: List[Tuple[np.ndarray, np.ndarray]] = []
    
    def add_observation(
        self,
        obs: np.ndarray,
    ) -> Tuple[np.ndarray, np.ndarray]:
        """
        Add observation and get its masks.
        
        Args:
            obs: Observation
        
        Returns:
            Tuple of (fg_mask, bg_mask)
        """
        fg_mask, bg_mask = self.processor.process_observation(obs)
        
        self._obs_buffer.append(obs)
        self._mask_buffer.append((fg_mask, bg_mask))
        
        # Trim buffer if needed
        if len(self._obs_buffer) > self.buffer_size:
            self._obs_buffer.pop(0)
            self._mask_buffer.pop(0)
        
        return fg_mask, bg_mask
    
    def get_episode_masks(self) -> Tuple[np.ndarray, np.ndarray]:
        """
        Get all masks for current episode.
        
        Returns:
            Tuple of (fg_masks, bg_masks)
        """
        if not self._mask_buffer:
            return np.array([]), np.array([])
        
        fg_masks = np.stack([m[0] for m in self._mask_buffer])
        bg_masks = np.stack([m[1] for m in self._mask_buffer])
        
        return fg_masks, bg_masks
    
    def reset(self):
        """Reset for new episode."""
        self._obs_buffer.clear()
        self._mask_buffer.clear()

